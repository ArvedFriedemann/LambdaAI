search function that evaluates a term. Applied to itself to evaluate itself as well


pair = /a/b/f f a b
fst = /p p(/a/b a)
snd = /p p(/a/b b)

true = /a/b a
false = /a/b b
not = /p p(false)(true)
or = /x/y x(true)y
and = /x/y x y (false)

type {1..n} = /1../n i (for the ith constructor)

to function (tf):
variable k = /x1../xn xk <- needs to have the deBrujin distance to quantifier (to retrieve the correct var)
application = /x (tf (fst x))(tf (snd x))
abstraction = /x (/v (tf snd x) v) (fst x)

start with axioms
they have to be strong enough for eval function, transforming own representation and so on.
apply to itself (helps with nondeterminimsm)
criteria for speed improvement (e.g. there had been a shorter way), prolly weaved into eval function.

let s:T->(d,T) (T is set of terms, d is index list of reductions) be a function
evaluating a lambda term optimally (shortest path to termination, d minimal).
then construct a lambda term searching for its implementation and run it on the algorithm.
Recurse until something meaningful happens, fill in nondet variables randomly (?)

---------------------------------------------------------------------------
first of all, a function eval with
eval <eval> = eval --should follow from other axioms
eval <var x> = <var x>
eval <(/x E)y> = eval <E[x|y]>
eval <E E'> = eval <apply (eval <E>) (eval <E'>)> --here is a CHOICE! (which one to evaluate first)
eval <(/x E)> = eval <abstr x <eval <E>>
--and maybe some robustness to alpha conversion
of course eval is then recursed eval(eval(eval ...)) BUT, for improvement,
the implementation of eval can change during that recursion!
In fact, an improvement more looks like eval <eval <E>> = (eval', E'), eval' <eval' <E'>> = ...

speed improvement: eval<eval <E>> = (eval', E') iff not exists evalB /= eval with less steps

now nondeterminism needs to be made explicit
(eval choices <E>) is deterministic
speed improvement': eval<eval <E>> = eval choices <E> iff not exists choicesB /= choices with (eval choicesB <E>) taking less steps

deterministic evaluation with equivalence:
for all functions eval being an evaluation function, if for the function eval' holds
that forall x. eval<eval' <x>> = eval<x>, then eval' is also an evaluation function.
strengthens the speed improvement: eval <eval <E>> = eval <E> iff there is no eval' with less steps

--------------------------------------------------------------------------
axiom system:
forall x.P(x) `eq` eval <enum x and conjunct>
induction should follow from this


IDEA:
evaluation: eagerly evaluate all possible points and then shrink equalities.
Also but include own representation. Does that work? Will it be faster?
Well...two terms might me equal at different points...hm...

Choice operator for lists! Implemented to speed up the search!

---------------------------------------------------------------------------
laws of common AI:
underlying structure: lambda calculus

eval acc (Application a b) = (eval f a) (eval f b)
eval acc (Abstraction x lx) = /y (eval (y:acc) lx)
eval acc (Variable x) = x acc      --variables need to be selectors (deBrujin style)

inductive principle:
ind f a0 succ = (f a0) and (ind f (succ a) succ) == forall a in (iterate a0 succ). f a    --just follows from definition of forall

set of all expressions:              --for better implementations see haskell
lamdas = lams succ (Variable 0)
lams succ (Variable x) = (Variable <$> (iterate x succ)) ++ [Abstraction e0 (Variable e0)] --e0 element not needed when using DeBrujin and renaming
lams succ (Abstraction x lx) = ((Abstraction x) <$> (iterate lx (lams succ))) ++ [Application e0 e0]
lams succ (Application a b) = Application (iterate a (lams succ)) (iterate b (lams succ)) --and add that the carthesian product is used obv.

now expressions like "forall programs. something something" are possible

determinism (leftmost outermost):
evalDet (DONE, x) = x
evalDet (_, a@(Abstraction x lx)) = case evalDet lx of (DONE, lx') then lx' else eval a
evalDet (_, (Application x y)) = case evalDet x of (DONE, x') then (DONE, Application x' y) else (Application x) <$> (evalDet y) --for <$> being application on snd argument
evalDet (_, Variable x) = (UNDONE, Variable x)

isEvalDet l = forall q in lambdas. (evalDet l) q = eval q

speedup:
x y = flip y x (by leftmost outermost y is evaluated first (x otherwise))

generalized:
eval = evalDet.change
exists change in lambdas with eval = evalDet.change               --first claim -> might be extractable proof

lemma:
proofLength x f = if (x = f x) then 0 else 1 + (proofLength (f x) f)

stronger:
  exists change in lambdas with (eval = evalDet.change) and
    forall change' in lambdas with (eval = evalDet.change) and
      forall x in lambdas (prooflength $ (evalDet.change) x) <= (prooflength $ (evalDet.change'.(find-joint-implementation)) x)

As a "lazy Proof" (one, that is not completed even in an implementation), this formulates a constantly improving solver
The find-joint-implementation means, that the solver can "choose" between the two algorithms and use the "fastest" one


--TODO: it should just be better than running the two solvers optimally in parallel!
---------------------------------------
generalized halting:
evalH::Lambda a -> (Bool, Lambda a)
evalHStep::Lambda a -> ({True, False, DK}, Lambda a)
implemented by general concept of warping goedel machines
Note: should technically follow from speed criterium. If machine doesn't do induction it will always be slower than one that does
---------------------------------------

intermezzo: automated search by contraditions (clause learning lambda equivalence):
fastestComp (Application x y) = minBy (proofLength.evalDet) [x y, flip y x]
...
learn = forall l::[(a,b)]. exists f in lambdas. forall (x,y) in l. (f x) = y
Maybe even add "(evalDet l) x = y"
learn (graph fastestComp)      <- creating the speedup (but forgetting the cost of speedup)

perfect construction:
exists l in lambdas. (isEvalDet l) and (not exists l' in lambdas. (isEvalDet l') and TODO)

implementation:
need an interface between what is automatically executed and what is subject to change
(so that known metafeatures are just operated). Prolly parallelism is needed.

----------------------------------
Quick thought: machines need to emulate non haltings forever. Fix: Check in quicker version.
If those wrong halting answer must have been wrong


-----------------------------------------------------
lambda representation in lambda calculus:

true = /a/b a
false = /a/b b
and = /a/b a b false
or = /a/b a true b
not = /a a false true

tuple = /a/b/f f a b
fst = /t t (/a/b a)
snd = /t t (/a/b b)

[] = false
: = tuple
head = fst
tail = snd
null = /l l (/a/b false) true


------------------------------------------------------------
Making Lambda Readable

if a lambda term is compressed to its own smallest representation, it has the form
(/fkt1/fkt2/fkt3) fktdef1 fktdef2 ...
where all these functions are again (kinda) their smallest representations. They can be renamed with
library functions. for readability, the inner workings of the reader need to be given (or assumed
upon the laws of common AI)
